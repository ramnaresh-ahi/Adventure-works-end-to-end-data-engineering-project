# Adventure Works Data Engineering Project

## ğŸ“‹ Overview

End-to-end data pipeline demonstrating modern data engineering practices using AWS services. 
Extracts retail data from Kaggle, transforms it using AWS Glue, and loads into Redshift Serverless 
for analytics.

**Status:** âœ… Complete | **Timeline:** October 2025

---

## ğŸ—ï¸ Architecture

[Insert architecture diagram here]

### Data Flow
1. **Data Ingestion** â†’ AWS Lambda extracts from Kaggle API
2. **Data Discovery** â†’ AWS Glue Crawler catalogs raw data
3. **Data Transformation** â†’ Glue ETL job transforms using PySpark
4. **Data Warehouse** â†’ Data loaded into Redshift Serverless
5. **Analytics Ready** â†’ Star schema ready for BI tools

---

## ğŸ› ï¸ Technologies & Services Used

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Data Ingestion** | AWS Lambda, Python | Extract Kaggle data |
| **Data Catalog** | AWS Glue Crawler | Schema discovery |
| **ETL Processing** | AWS Glue, PySpark | Transform & clean data |
| **Data Lake** | Amazon S3 | Store raw & transformed data |
| **Data Warehouse** | Redshift Serverless | Analytics-ready warehouse |
| **Infrastructure** | IAM, VPC, Security Groups | AWS configuration |
| **Language** | Python, SQL | Scripting & queries |

---

## ğŸ“Š Key Achievements

âœ… **End-to-End Pipeline** - Complete data flow from source to warehouse  
âœ… **Data Modeling** - Star schema with 5+ dimension tables  
âœ… **Data Quality** - Handled schema mismatches, data type conversions  
âœ… **Cloud Infrastructure** - Configured VPC, security groups, IAM roles  
âœ… **Scalable Design** - Serverless architecture for cost optimization  
âœ… **Problem Solving** - Debugged and resolved connectivity & data issues  

---

## ğŸ“ Project Structure
```
adventure-works-data-engineering/
â”‚
â”œâ”€â”€ README.md 
â”œâ”€â”€ ARCHITECTURE.md (Architecture & design decisions)
â”œâ”€â”€ LICENSE
â”‚
â”œâ”€â”€ 01_data_ingestion/
â”‚   â”œâ”€â”€ lambda_function.py (Data extraction from Kaggle)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md (Lambda setup instructions)
â”‚
â”œâ”€â”€ 02_data_discovery/
â”‚   â”œâ”€â”€ glue_crawler_config.json
â”‚   â””â”€â”€ README.md (Crawler setup instructions)
â”‚
â”œâ”€â”€ 03_data_transformation/
â”‚   â”œâ”€â”€ glue_etl_job.py (PySpark transformation script)
â”‚   â”œâ”€â”€ glue_job_config.json
â”‚   â””â”€â”€ README.md (ETL job documentation)
â”‚
â”œâ”€â”€ 04_data_warehouse/
â”‚   â”œâ”€â”€ redshift_schema.sql (DDL for dimension/fact tables and data loading)
â”‚   â””â”€â”€ README.md (Redshift setup)
â”‚
â”œâ”€â”€ 05_infrastructure/
â”‚   â”œâ”€â”€ iam_policy.json (IAM roles & policies)
â”‚   â”œâ”€â”€ security_groups.json
â”‚   â””â”€â”€ README.md (Infrastructure setup)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ data_flow_diagram.png
â”‚   â””â”€â”€ schema_design.png
â”‚
â””â”€â”€ images/
    â””â”€â”€ (screenshots of each stage)
```
See each folder's README for detailed setup instructions.

---

## ğŸš€ Quick Start

### Prerequisites
- AWS Account with appropriate permissions
- Python 3.8+
- AWS CLI configured

### Setup Steps

1. **Configure AWS Credentials**


2. **Deploy Lambda Function**
See `01_data_ingestion/README.md`

3. **Create Glue Crawler**
See `02_data_discovery/README.md`

4. **Deploy Glue ETL Job**
See `03_data_transformation/README.md`

5. **Create Redshift Tables**
See `04_data_warehouse/README.md`

---

## ğŸ“ˆ Data Model

### Star Schema Design

**Fact Table:** `fact_sales`
- Measures: order quantity, unit price, total amount, total cost
- Dimensions: customer, product, territory, date

**Dimension Tables:**
- `dim_customers` (10,000+ records)
- `dim_products` (500+ records)
- `dim_territories` (10+ records)
- `product_categories` & `product_subcategories`

---

## ğŸ”‘ Key Features

### 1. Data Ingestion (Lambda)
- Extracts Adventure Works dataset from Kaggle API
- Automatically loads to S3 `raw/` folder
- Scheduled execution via CloudWatch Events

### 2. Data Discovery (Glue Crawler)
- Automatically discovers schema from Parquet files
- Populates Glue Data Catalog
- Enables Athena & other AWS services to query data

### 3. Data Transformation (Glue ETL)
- Merges multiple sales tables
- Handles data type conversions (string â†” date, int)
- Removes duplicates & null values
- Outputs Parquet format to S3 `transformed/` folder

### 4. Data Warehouse (Redshift)
- Implements star schema for analytics
- Optimized for OLAP queries
- Supports complex joins across dimensions

---

## ğŸ¯ Challenges & Solutions

| Challenge | Solution |
|-----------|----------|
| Data type mismatches | Custom PySpark transformations with explicit casting |
| Schema discovery | AWS Glue Crawler automation |
| Merging multiple tables | DynamicFrameCollection to single DataFrame |
| S3 to Redshift loading | COPY commands with IAM role authentication |
| Networking issues | VPC security group & public endpoint configuration |

---

## ğŸ“Š Data Statistics

- **Total Raw Data:** ~500k+ records
- **Source Tables:** 10 tables (sales, customers, products, etc.)
- **Transformation Rules:** 20+ data quality checks
- **Final Records in Warehouse:** ~500k+ optimized rows
- **Compression:** Parquet format provides 80%+ compression vs CSV

---

## ğŸ’¡ Design Decisions

### Why Star Schema?
âœ… Optimized for analytical queries  
âœ… Simplified joins  
âœ… Fast aggregations  

### Why Serverless Redshift?
âœ… Auto-scaling based on workload  
âœ… Pay-per-use pricing  
âœ… No cluster management  

### Why Parquet Format?
âœ… Columnar storage for compression  
âœ… Schema evolution support  
âœ… Compatible with Spark & Athena  

---

## ğŸ” Security & Permissions

- **IAM Roles:** Principle of least privilege
- **VPC Configuration:** Private subnets with security groups
- **Data Encryption:** S3 encryption enabled
- **Access Control:** Role-based access for Glue jobs

See `05_infrastructure/` for detailed IAM policies.

---

## ğŸ“š Project Learnings

### AWS Services Mastered
- Lambda for serverless compute
- Glue for ETL orchestration
- S3 for data lake storage
- Redshift for data warehousing
- IAM for access management

### Data Engineering Concepts Applied
- ETL pipeline design
- Star schema modeling
- Data quality transformations
- Cloud infrastructure setup
- Troubleshooting & debugging

---

## ğŸ“ Future Enhancements

- [ ] Add data quality metrics & monitoring
- [ ] Implement incremental loading (CDC)
- [ ] Create data lineage documentation
- [ ] Add cost optimization analysis
- [ ] Implement automated testing
- [ ] Add monitoring & alerting dashboard

---

## ğŸ“ Contact & Questions

For questions about this project, [Connect with me on LinkedIn](https://www.linkedin.com/in/ramnaresh-ahirwar-77abc/)

---

## ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

---

## ğŸ™ Acknowledgments

- Kaggle for Adventure Works dataset
- AWS documentation & best practices
- Open-source Python/PySpark communities


